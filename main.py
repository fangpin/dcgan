import tensorflow as tf
import tensorlayer as tl
import numpy as np
import os
import time
import sys
from model import generator, discriminator, CFG
from util import *
from glob import glob
from random import shuffle


def main():
    # global config
    batch_size = CFG.batch_size
    img_size = CFG.img_size
    channels = CFG.channels
    z_dim = CFG.z_dim
    sample_size = CFG.sample_size
    lr = CFG.lr
    beta1 = CFG.beta1
    checkpoint_dir = CFG.checkpoint_dir
    sample_dir = CFG.sample_dir
    dataset = CFG.dataset
    epoch = CFG.epoch
    crop_size = CFG.crop_size
    is_crop = CFG.is_crop
    sample_step = 500
    save_step = 500

    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
    if not os.path.exists(sample_dir):
        os.makedirs(sample_dir)

    z = tf.placeholder(tf.float32, [batch_size, z_dim],
                       name='noise_input')
    real_imgs = tf.placeholder(tf.float32, [batch_size, img_size, img_size, channels], name='real_imgs')

    # generate fake imgs from noise z
    g, g_logits = generator(z, is_train=True, reuse=False)
    # discriminator for fake imgs generated by generator
    d, d_logits = discriminator(g.outputs, is_train=True, reuse=False)
    # discriminator for real imgs
    d2, d2_logits = discriminator(real_imgs, is_train=True, reuse=True)
    # generator for evaluation,shaofeng   is_train false
    g2, g2_logits = generator(z, is_train=False, reuse=True)

    # loss functions
    d_loss_real = tl.cost.sigmoid_cross_entropy(d2_logits, tf.ones_like(d2_logits),
                                                name='d_real')
    d_loss_fake = tl.cost.sigmoid_cross_entropy(d_logits, tf.zeros_like(d_logits),
                                                name='d_fake')
    d_loss = d_loss_real + d_loss_fake
    g_loss = tl.cost.sigmoid_cross_entropy(d_logits, tf.ones_like(d_logits),
                                           name='g_loss')

    # trainable variables
    g_vars = tl.layers.get_variables_with_name('generator', True, True)
    d_vars = tl.layers.get_variables_with_name('discriminator', True, True)

    # train operators
    train_d = tf.train.AdamOptimizer(lr, beta1=beta1).minimize(d_loss, var_list=d_vars)
    train_g = tf.train.AdamOptimizer(lr, beta1=beta1).minimize(g_loss, var_list=g_vars)

    sess = tf.InteractiveSession()
    sess.run(tf.global_variables_initializer())

    model_name = '%s_%s_%s' % (dataset, batch_size, img_size)
    save_dir = os.path.join(checkpoint_dir, model_name)
    tl.files.exists_or_mkdir(save_dir)

    # load the latest checkpoint
    g_name = os.path.join(save_dir, 'g.npz')
    d_name = os.path.join(save_dir, 'd.npz')

    data_files = glob(os.path.join('data', dataset, '*.jpg'))
    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, z_dim)).astype(np.float32)

    # train models
    iter_counter = 0
    for ep in range(epoch):
        shuffle(data_files)
        print('Dataset shuffled')

        sample_files = data_files[0: sample_size]
        samples = [get_img(sample, crop_size=crop_size, is_crop=is_crop, resize_w=img_size) for sample in sample_files]
        sample_img = np.array(samples).astype(np.float32)
        print('sample images updated')

        batch_num = len(data_files) // batch_size

        for id in range(batch_num):
            batch_files = data_files[id*batch_size:(id+1)*batch_size]
            batchs = [get_img(batch_file, crop_size, is_crop=is_crop, resize_w=img_size) for batch_file in batch_files]
            batch_imgs = np.array(batchs).astype(np.float32)
            batch_z = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, z_dim)).astype(np.float32)
            start_time = time.time()
            d_loss_val, _ = sess.run([d_loss, train_d], feed_dict={z: batch_z, real_imgs: batch_imgs})

            for _ in range(2):
                g_loss_val, _ = sess.run([g_loss, train_g], feed_dict={z:batch_z})
            print("Epoch: [%2d/%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f" \
                    % (ep, epoch, id, batch_num,
                        time.time() - start_time, d_loss_val, g_loss_val))
            sys.stdout.flush()

            iter_counter += 1
            if np.mod(iter_counter, sample_step) == 0:
                # generate and visualize generated images
                img, errD, errG = sess.run([g2.outputs, d_loss, g_loss], feed_dict={z : sample_seed, real_imgs: sample_img})
                '''
                img255 = (np.array(img) + 1) / 2 * 255
                tl.visualize.images2d(images=img255, second=0, saveable=True,
                                name='./{}/train_{:02d}_{:04d}'.format(FLAGS.sample_dir, epoch, idx), dtype=None, fig_idx=2838)
                '''
                save_images(img, [8, 8],
                            './{}/train_{:02d}_{:04d}.png'.format(sample_dir, ep, id))
                print("[Sample] d_loss: %.8f, g_loss: %.8f" % (errD, errG))
                sys.stdout.flush()

            if np.mod(iter_counter, save_step) == 0:
                # save current network parameters
                print("[*] Saving checkpoints...")
                img, errD, errG = sess.run([g2.outputs, d_loss, g_loss], feed_dict={z : sample_seed, real_imgs: sample_img})
                model_dir = "%s_%s_%s" % (dataset, batch_size, img_size)
                save_dir = os.path.join(checkpoint_dir, model_dir)
                if not os.path.exists(save_dir):
                    os.makedirs(save_dir)
                # the latest version location
                net_g_name = os.path.join(save_dir, 'net_g.npz')
                net_d_name = os.path.join(save_dir, 'net_d.npz')
                # this version is for future re-check and visualization analysis
                net_g_iter_name = os.path.join(save_dir, 'net_g_%d.npz' % iter_counter)
                net_d_iter_name = os.path.join(save_dir, 'net_d_%d.npz' % iter_counter)
                tl.files.save_npz(g.all_params, name=net_g_name, sess=sess)
                tl.files.save_npz(d.all_params, name=net_d_name, sess=sess)
                tl.files.save_npz(g.all_params, name=net_g_iter_name, sess=sess)
                tl.files.save_npz(d.all_params, name=net_d_iter_name, sess=sess)
                print("[*] Saving checkpoints SUCCESS!")

    sess.close()


if __name__ == '__main__':
    main()
